{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af154b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "t = torch.linspace(-10, 10, steps=10000)\n",
    "\n",
    "# compute x(t) and y(t) as defined above\n",
    "x = t - 1.5 * (15*t).cos()\n",
    "y = t - 1.5 * (14*t).sin()\n",
    "\n",
    "plt.plot(x.numpy(), y.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0edf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boston house price\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "plt.scatter(boston.data[:, -1], boston.target)\n",
    "\n",
    "boston.data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression with PyTorch\n",
    "\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(boston.data[:, -1] / 10, dtype=torch.float32)\n",
    "y = torch.tensor(boston.target, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eceeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w * x + b\n",
    "loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "# propagate gradients\n",
    "loss.backward()\n",
    "print(\"dL/dw = \\n\", w.grad)\n",
    "print(\"dL/db = \\n\", b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for i in range(200):\n",
    "    y_pred = w * x + b\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights, with lr == 0.05\n",
    "    w.data -= 0.05 * w.grad.data\n",
    "    b.data -= 0.05 * b.grad.data\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    # the rest of code is just bells and whistles\n",
    "    if (i + 1) % 5 == 0:\n",
    "        clear_output(True)\n",
    "        plt.scatter(x.numpy(), y.numpy())\n",
    "        plt.scatter(x.numpy(), y_pred.detach().numpy(), color='orange', linewidth=5)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.detach().numpy())\n",
    "        if loss.detach().numpy() < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223fa3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png is broken\n",
      "File F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png is broken\n"
     ]
    }
   ],
   "source": [
    "from DataLoader import notMNIST\n",
    "dataset = notMNIST(\"/Users/czkaiweb/Research/DeepVision/dataset/notMNIST_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c3f56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using DataLoader as generator to get item from notMNIST dataset\n",
    "from torch.utils.data import DataLoader,Sampler\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "def display():\n",
    "    for step, data in enumerate(train_loader,0):\n",
    "        train_x, train_y = data\n",
    "        plt.imshow(train_x[0].reshape([28, 28]))\n",
    "        if step == 0:\n",
    "            break\n",
    "        \n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5981f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simpler way:\n",
    "#from torchvision import datasets,transforms\n",
    "#dataloader = datasets.ImageFolder(root=\"/Users/czkaiweb/Research/DeepVision/dataset/notMNIST_small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6562f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base class for all neural network modules.\n",
      "\n",
      "    Your models should also subclass this class.\n",
      "\n",
      "    Modules can also contain other Modules, allowing to nest them in\n",
      "    a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "        import torch.nn as nn\n",
      "        import torch.nn.functional as F\n",
      "\n",
      "        class Model(nn.Module):\n",
      "            def __init__(self):\n",
      "                super(Model, self).__init__()\n",
      "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "            def forward(self, x):\n",
      "                x = F.relu(self.conv1(x))\n",
      "                return F.relu(self.conv2(x))\n",
      "\n",
      "    Submodules assigned in this way will be registered, and will have their\n",
      "    parameters converted too when you call :meth:`to`, etc.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "## Start with layers\n",
    "from torch import nn\n",
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441a02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Model\n",
    "\n",
    "model = Model()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crossentropy = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61047ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Erdos/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/anaconda3/envs/Erdos/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/envs/Erdos/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/czkaiweb/Research/DeepVision/Model.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.lin2(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss 2.003589790161342\n",
      "Epoch 20, loss 1.8347971186245957\n",
      "Epoch 30, loss 1.7652716293726882\n",
      "Epoch 40, loss 1.6603884394854715\n",
      "Epoch 50, loss 1.6219551636748117\n",
      "Epoch 60, loss 1.6085170589081228\n",
      "Epoch 70, loss 1.6000593247478956\n",
      "Epoch 80, loss 1.594302477901929\n",
      "Epoch 90, loss 1.5894361922185716\n",
      "Epoch 100, loss 1.585938373657122\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "def train(epoch):\n",
    "    maxstep = 0\n",
    "    epoch_loss = 0\n",
    "    n_batches = len(dataset)//128\n",
    "    for step,data in enumerate(train_loader,0):\n",
    "        train_x, train_y = data\n",
    "        train_x  = train_x.reshape(-1,784)\n",
    "        x = torch.tensor(train_x,dtype=torch.float32)\n",
    "        y = torch.tensor(train_y,dtype=torch.float32)\n",
    "        y_pred = model.forward(x)\n",
    "        y_pred = y_pred.view(-1,10)\n",
    "        # compute loss\n",
    "        loss = crossentropy(y_pred,y.long())\n",
    "        epoch_loss += loss.item()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # adam step\n",
    "        opt.step()\n",
    "        \n",
    "        # clear gradients\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        if step % n_batches == 0 and step != 0 and epoch%10 == 0:\n",
    "            epoch_loss = epoch_loss / n_batches\n",
    "            loss_history.append(epoch_loss)\n",
    "            print(\"Epoch {}, loss {}\".format(epoch, epoch_loss))\n",
    "            epoch_loss = 0\n",
    "            \n",
    "for epoch in range(1, 101):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fc1fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy with test sample: 0.90234375\n"
     ]
    }
   ],
   "source": [
    "## Evaluate \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        img,label = data\n",
    "        img  = img.reshape(-1,784)\n",
    "        output = model.forward(img)\n",
    "        _,predict = torch.max(output.data,1)\n",
    "        filterList = (predict == label)\n",
    "        correct += filterList.sum().data\n",
    "        total   += len(filterList)\n",
    "        \n",
    "        if i == 1:\n",
    "            break\n",
    "print(\"accurancy with test sample: {}\".format(float(correct)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e89a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89a952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('Erdos': conda)",
   "language": "python",
   "name": "python371064biterdosconda7530abd590984c45ba6372cad090cba4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
